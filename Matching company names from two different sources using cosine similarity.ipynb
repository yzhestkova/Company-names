{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names matching can be tricky. In many cases, different datasets have slighly difference spelling of the same person's or the same company's name. Patent data also quite often have typos in names. This code utilizes cosine similarity with n-grams approach to match company names from two different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "import numpy as np\n",
    "#import spacy\n",
    "import string\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading the first dataset with names for the match (in my example, Compustat Capital IQ)\n",
    "compustat=pd.read_csv('compustat.csv')\n",
    "\n",
    "#Basic cleaning and making sure both vectors of names are formatted in the same way\n",
    "compustat['conm']=compustat['conm'].str.lower()\n",
    "compustat=compustat.rename(columns={'conm':'comp_name'})\n",
    "compustat['comp_name']=compustat['comp_name'].str.strip()\n",
    "compustat['comp_name']=compustat['comp_name'].astype(str)\n",
    "compustat['comp_name']=compustat['comp_name'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "compustat['length']=compustat['comp_name'].str.len()\n",
    "compustat=compustat[compustat['length']>5]\n",
    "compustat=compustat.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading the second dataset with names for the match (in my example, Assignees from USPTO patent data)\n",
    "assignees=pd.read_csv('assignees_list.csv')\n",
    "\n",
    "#Basic cleaning and making sure both vectors of names are formatted in the same way\n",
    "assignees['ass_name']=assignees['ass_name'].str.lower()\n",
    "assignees['ass_name']=assignees['ass_name'].str.strip()\n",
    "assignees['ass_name']=assignees['ass_name'].astype(str)\n",
    "assignees['ass_name']=assignees['ass_name'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "assignees['length']=assignees['ass_name'].str.len()\n",
    "assignees=assignees[assignees['length']>5]\n",
    "assignees=assignees.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "####### Pieces of the following code related to fast computation of cosine similarity matrix are taken from van den Blog: https://bergvca.github.io/2017/10/14/super-fast-string-matching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining your n-gram that you will use as a unit of vocabulary\n",
    "def ngrams(string, n=4):\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "#Uploading the TF-IDF library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Combining names from both datasets in one np array\n",
    "all_names=np.concatenate((compustat['comp_name'],assignees['ass_name']))\n",
    "\n",
    "#Vectorization - dividing all names into 4-grams\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct # use pip install sparse_dot_topn if you don't have it\n",
    "\n",
    "#Function that computes cossine similarity and saves only ntop similarity pairs by score\n",
    "#If similarity score is lower than lower_bound, a pair is ignored\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For names, I would put the lower_bound pretty high\n",
    "import time\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.8)\n",
    "t = time.time()-t1\n",
    "print(\"SELFTIMED:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe. If letting top parameter to kick in, you can first check what the subsample looks like\n",
    "#That is helpful when trying to understand the optimal lower_bound\n",
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    #if top:\n",
    "     #   nr_matches = top\n",
    "    #else:\n",
    "     #   nr_matches = sparsecols.size\n",
    "    \n",
    "    nr_matches = sparsecols.size\n",
    "\n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'ass_name': left_side,\n",
    "                          'comp_name': right_side,\n",
    "                           'similarity': similairity})\n",
    "\n",
    "matches_df = get_matches_df(matches, all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The matrix creates similarity score for all possible pairs in the vector of combined names\n",
    "#You need to make sure that one column has only names from dataset 1 and the other column has only names from dataset 2\n",
    "assignees=assignees[['ass_name']]\n",
    "compustat=compustat[['comp_name']]\n",
    "matches_df=pd.merge(matches_df, assignees, how='inner', left_on='ass_name', right_on='ass_name')\n",
    "matches_df=pd.merge(matches_df, compustat, how='inner', left_on='comp_name', right_on='comp_name')\n",
    "matches_df=matches_df.drop_duplicates()\n",
    "#You might want to double check that the companies in the pairs start with the same letters\n",
    "matches_df['ass_first']=matches_df['ass_name'].str[0:3]\n",
    "matches_df['comp_first']=matches_df['comp_name'].str[0:3]\n",
    "matches_df=matches_df[matches_df['ass_first']==matches_df['comp_first']]\n",
    "matches_df=matches_df[['comp_name','ass_name','similarity']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
